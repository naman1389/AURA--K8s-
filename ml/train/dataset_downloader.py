"""
BEAST LEVEL Dataset Downloader
Downloads real datasets from Kaggle, GitHub, and other sources for training
Optimized for Mac M4 - downloads and processes in chunks
"""

import os
import sys
import json
import requests
import zipfile
import tarfile
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Optional, List, Dict, Tuple
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Configuration
DATASET_DIR = Path("datasets")
DATASET_DIR.mkdir(exist_ok=True)

# Dataset sources - ULTIMATE PEAK PERFORMER: 20+ Datasets
DATASET_SOURCES = {
    # Existing datasets
    "yahoo_s5": {
        "name": "Yahoo S5 Time Series Anomaly Detection",
        "url": "https://github.com/datamllab/tods/raw/master/datasets/anomaly_detection/yahoo.zip",
        "type": "time_series",
        "description": "Real and synthetic time-series data with labeled anomalies",
        "alternative": "https://raw.githubusercontent.com/yzhao062/pyod/master/examples/yahoo_s5.zip",
        "priority": "tier1"
    },
    "numenta": {
        "name": "Numenta Anomaly Benchmark",
        "url": "https://github.com/numenta/NAB/archive/refs/heads/master.zip",
        "type": "time_series",
        "description": "Real-time anomaly detection benchmark dataset",
        "priority": "tier1"
    },
    "kdd_cup_99": {
        "name": "KDD Cup 1999 Network Intrusion",
        "url": "https://www.kaggle.com/datasets/galaxyh/kdd-cup-1999-data/download?datasetVersionNumber=1",
        "type": "network",
        "description": "Network intrusion detection dataset (can be adapted for K8s)",
        "kaggle": "galaxyh/kdd-cup-1999-data",
        "priority": "tier1"
    },
    "synthetic_k8s": {
        "name": "Synthetic Kubernetes Metrics",
        "url": None,
        "type": "synthetic",
        "description": "Generate synthetic Kubernetes-like metrics",
        "priority": "tier1"
    },
    # NEW Tier 1 Datasets - Immediate Implementation
    "ai4i_2020": {
        "name": "AI4I 2020 Predictive Maintenance",
        "url": "https://archive-beta.ics.uci.edu/dataset/601/ai4i+2020+predictive+maintenance+dataset",
        "type": "predictive_maintenance",
        "kaggle": "stephenpolozoff/ai4i-2020-predictive-maintenance-dataset",
        "description": "Industrial predictive maintenance with failure labels (10K samples, 14 features)",
        "priority": "tier1"
    },
    "secom": {
        "name": "SECOM Manufacturing Dataset",
        "url": "https://archive-beta.ics.uci.edu/dataset/179/secom",
        "type": "manufacturing",
        "description": "Semiconductor manufacturing sensor data (590 sensors, fault detection)",
        "priority": "tier1"
    },
    "pump_sensor": {
        "name": "Pump Sensor Data",
        "url": "https://www.kaggle.com/datasets/nphantawee/pump-sensor-data",
        "type": "predictive_maintenance",
        "kaggle": "nphantawee/pump-sensor-data",
        "description": "Water pump sensor multivariate time-series for predictive maintenance",
        "priority": "tier2"
    },
    "unsw_nb15": {
        "name": "UNSW-NB15 Network Intrusion Dataset",
        "url": "https://www.kaggle.com/datasets/mrwellsdavid/unsw-nb15",
        "type": "network",
        "kaggle": "mrwellsdavid/unsw-nb15",
        "description": "Comprehensive network attack patterns for network anomaly detection",
        "priority": "tier2"
    },
    # Note: Real-IAD, NASA Engine, CMAPSS, TS-AD datasets require specialized access
    # These will be implemented with fallback to synthetic generation
    "nasa_cmapss": {
        "name": "NASA CMAPSS Engine Dataset",
        "url": None,  # Requires special access, will use alternative
        "type": "predictive_maintenance",
        "kaggle": "behrad3d/nasa-cmaps",  # Alternative source
        "description": "Turbofan engine simulation data with multiple fault modes",
        "priority": "tier1"
    },
    # Additional time-series anomaly detection datasets
    "ts_ad_servers": {
        "name": "TS-AD Application Server Datasets",
        "url": "https://github.com/elisejiuqizhang/TS-AD-Datasets",
        "type": "time_series",
        "description": "Application server time-series anomaly detection datasets",
        "priority": "tier1"
    },
    # Cloud/Virtual Machine datasets
    "azure_vm_metrics": {
        "name": "Azure VM Monitoring Metrics",
        "url": None,  # Public dataset access varies
        "type": "cloud_vm",
        "description": "Azure VM and container metrics for Kubernetes cloud deployments",
        "priority": "tier2"
    },
    # Note: ADBenchmarks repository contains 100+ datasets
    # We'll implement support for downloading from that repository
    "adbenchmarks_sample": {
        "name": "ADBenchmarks Sample Datasets",
        "url": "https://github.com/GuansongPang/anomaly-detection-datasets",
        "type": "mixed",
        "description": "Sample datasets from ADBenchmarks collection (100+ datasets available)",
        "priority": "tier1",
        "note": "Large collection - will download subsets"
    }
}


class DatasetDownloader:
    """
    Downloads and processes datasets for Kubernetes anomaly detection
    Supports multiple sources: Kaggle, GitHub, direct URLs
    """
    
    def __init__(self, dataset_dir: Path = DATASET_DIR):
        self.dataset_dir = dataset_dir
        self.dataset_dir.mkdir(exist_ok=True, parents=True)
        
    def download_dataset(self, dataset_name: str, force: bool = False) -> Path:
        """
        Download a dataset by name
        
        Args:
            dataset_name: Name of dataset (yahoo_s5, numenta, kdd_cup_99, synthetic_k8s)
            force: Force re-download even if exists
            
        Returns:
            Path to downloaded dataset directory
        """
        if dataset_name not in DATASET_SOURCES:
            raise ValueError(f"Unknown dataset: {dataset_name}. Available: {list(DATASET_SOURCES.keys())}")
        
        dataset_info = DATASET_SOURCES[dataset_name]
        dataset_path = self.dataset_dir / dataset_name
        
        if dataset_path.exists() and not force:
            print(f"âœ… Dataset {dataset_name} already exists at {dataset_path}")
            return dataset_path
        
        print(f"ðŸ“¥ Downloading {dataset_info['name']}...")
        print(f"   Description: {dataset_info['description']}")
        
        if dataset_name == "synthetic_k8s":
            # Generate synthetic data
            return self._generate_synthetic_k8s_dataset(dataset_path)
        elif dataset_info.get("kaggle"):
            # Download from Kaggle (requires kaggle API)
            return self._download_from_kaggle(dataset_info["kaggle"], dataset_path, force)
        else:
            # Download from URL
            return self._download_from_url(dataset_info["url"], dataset_path, force)
    
    def _download_from_url(self, url: str, target_dir: Path, force: bool) -> Path:
        """Download dataset from URL"""
        target_dir.mkdir(exist_ok=True, parents=True)
        
        # Download file
        filename = url.split("/")[-1]
        filepath = target_dir / filename
        
        if filepath.exists() and not force:
            print(f"   File already exists: {filepath}")
        else:
            print(f"   Downloading from {url}...")
            try:
                response = requests.get(url, stream=True, timeout=60)
                response.raise_for_status()
                
                total_size = int(response.headers.get('content-length', 0))
                downloaded = 0
                
                with open(filepath, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            downloaded += len(chunk)
                            if total_size > 0:
                                percent = (downloaded / total_size) * 100
                                print(f"\r   Progress: {percent:.1f}%", end="", flush=True)
                print()  # New line after progress
                
            except Exception as e:
                print(f"   âš ï¸  Failed to download: {e}")
                print(f"   Generating synthetic fallback...")
                return self._generate_synthetic_k8s_dataset(target_dir)
        
        # Extract if archive
        if filename.endswith('.zip'):
            print(f"   Extracting {filename}...")
            with zipfile.ZipFile(filepath, 'r') as zip_ref:
                zip_ref.extractall(target_dir)
        elif filename.endswith(('.tar.gz', '.tgz')):
            print(f"   Extracting {filename}...")
            with tarfile.open(filepath, 'r:gz') as tar_ref:
                tar_ref.extractall(target_dir)
        elif filename.endswith('.tar'):
            print(f"   Extracting {filename}...")
            with tarfile.open(filepath, 'r') as tar_ref:
                tar_ref.extractall(target_dir)
        
        print(f"âœ… Dataset ready at {target_dir}")
        return target_dir
    
    def _download_from_kaggle(self, kaggle_dataset: str, target_dir: Path, force: bool) -> Path:
        """Download dataset from Kaggle (requires kaggle API)"""
        try:
            from kaggle.api.kaggle_api_extended import KaggleApi
        except ImportError:
            print("   âš ï¸  Kaggle API not installed. Install with: pip install kaggle")
            print("   Also need kaggle.json in ~/.kaggle/ directory or KAGGLE_USERNAME/KAGGLE_KEY env vars")
            print("   Falling back to synthetic dataset...")
            return self._generate_synthetic_k8s_dataset(target_dir)
        
        target_dir.mkdir(exist_ok=True, parents=True)
        
        try:
            print(f"   Downloading from Kaggle: {kaggle_dataset}...")
            
            # Initialize Kaggle API
            api = KaggleApi()
            
            # Authenticate - will use kaggle.json or environment variables automatically
            # Priority: 1) Environment variables (KAGGLE_USERNAME, KAGGLE_KEY)
            #           2) kaggle.json file (~/.kaggle/kaggle.json)
            api.authenticate()
            
            # Download dataset
            api.dataset_download_files(
                kaggle_dataset,
                path=str(target_dir),
                unzip=True,
                quiet=False
            )
            print(f"âœ… Kaggle dataset ready at {target_dir}")
            return target_dir
        except Exception as e:
            print(f"   âš ï¸  Failed to download from Kaggle: {e}")
            print("   Check your Kaggle credentials (kaggle.json or KAGGLE_USERNAME/KAGGLE_KEY)")
            print("   Falling back to synthetic dataset...")
            return self._generate_synthetic_k8s_dataset(target_dir)
    
    def _generate_synthetic_k8s_dataset(self, target_dir: Path, n_samples: int = 100000) -> Path:
        """
        Generate synthetic Kubernetes-like dataset with realistic anomalies
        Optimized for Mac M4 - generates in chunks
        """
        print(f"   Generating {n_samples} synthetic Kubernetes metrics samples...")
        target_dir.mkdir(exist_ok=True, parents=True)
        
        # 20+ anomaly types (expanded from 15)
        anomaly_types = [
            'healthy', 'cpu_spike', 'memory_leak', 'disk_full', 'network_latency',
            'pod_crash', 'oom_kill', 'slow_response', 'high_error_rate', 'deadlock',
            'resource_contention', 'connection_pool_exhausted', 'dns_resolution_failure',
            'disk_io_bottleneck', 'cache_thrashing', 'image_pull_error', 'crash_loop',
            'cpu_throttling', 'memory_pressure', 'network_partition', 'service_unavailable',
            'pod_eviction', 'node_not_ready', 'container_hang'
        ]
        
        # Generate in chunks to save memory
        chunk_size = 10000
        chunks = []
        
        np.random.seed(42)
        
        for chunk_idx in range(0, n_samples, chunk_size):
            chunk_data = []
            current_chunk_size = min(chunk_size, n_samples - chunk_idx)
            
            for _ in range(current_chunk_size):
                # Weighted random: 60% healthy, 40% anomalies
                anomaly = np.random.choice(
                    anomaly_types,
                    p=[0.6] + [0.4/(len(anomaly_types)-1)] * (len(anomaly_types)-1)
                )
                
                # Generate metrics based on anomaly type
                metrics = self._generate_anomaly_metrics(anomaly)
                metrics['anomaly_type'] = anomaly
                metrics['timestamp'] = datetime.now() - timedelta(seconds=np.random.randint(0, 86400 * 30))
                
                chunk_data.append(metrics)
                
                if (chunk_idx + _) % 10000 == 0:
                    print(f"   Generated {chunk_idx + _}/{n_samples} samples...", end="\r")
            
            chunks.append(pd.DataFrame(chunk_data))
            print(f"   Generated chunk {chunk_idx//chunk_size + 1}/{(n_samples-1)//chunk_size + 1}")
        
        # Combine chunks
        print("   Combining chunks...")
        df = pd.concat(chunks, ignore_index=True)
        
        # Shuffle
        df = df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        # Save to CSV
        output_file = target_dir / "synthetic_k8s_metrics.csv"
        df.to_csv(output_file, index=False)
        
        # Save metadata
        metadata = {
            "dataset_name": "synthetic_k8s",
            "generated_date": datetime.now().isoformat(),
            "total_samples": len(df),
            "anomaly_types": anomaly_types,
            "anomaly_distribution": df['anomaly_type'].value_counts().to_dict(),
            "features": df.columns.tolist()
        }
        
        with open(target_dir / "metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)
        
        print(f"âœ… Generated {len(df)} samples saved to {output_file}")
        return target_dir
    
    def _generate_anomaly_metrics(self, anomaly_type: str) -> Dict:
        """Generate metrics for a specific anomaly type"""
        base_metrics = {
            'cpu_usage': 0.0,
            'memory_usage': 0.0,
            'disk_usage': 0.0,
            'network_bytes_sec': 0.0,
            'error_rate': 0.0,
            'latency_ms': 0.0,
            'restart_count': 0,
            'age_minutes': 0.0,
            'cpu_limit': 1000.0,
            'memory_limit': 2147483648.0,  # 2GB
            'disk_limit': 10737418240.0,  # 10GB
            'cpu_trend': 0.0,
            'memory_trend': 0.0,
            'restart_trend': 0.0,
            'has_oom_kill': 0,
            'has_crash_loop': 0,
            'has_high_cpu': 0,
            'has_network_issues': 0,
            'ready': 1,
            'phase': 'Running',
            'container_state': 'Running'
        }
        
        if anomaly_type == 'healthy':
            base_metrics.update({
                'cpu_usage': np.random.uniform(10, 40),
                'memory_usage': np.random.uniform(20, 50),
                'disk_usage': np.random.uniform(10, 40),
                'network_bytes_sec': np.random.uniform(100, 500),
                'error_rate': np.random.uniform(0, 0.5),
                'latency_ms': np.random.uniform(10, 50),
                'restart_count': np.random.poisson(0.5),
                'age_minutes': np.random.uniform(1, 1440)
            })
        elif anomaly_type == 'cpu_spike':
            base_metrics.update({
                'cpu_usage': np.random.uniform(80, 100),
                'memory_usage': np.random.uniform(20, 60),
                'error_rate': np.random.uniform(0.5, 2),
                'latency_ms': np.random.uniform(50, 200),
                'has_high_cpu': 1,
                'cpu_trend': np.random.uniform(5, 20)
            })
        elif anomaly_type == 'memory_leak':
            base_metrics.update({
                'cpu_usage': np.random.uniform(30, 60),
                'memory_usage': np.random.uniform(70, 95),
                'error_rate': np.random.uniform(0.5, 3),
                'memory_trend': np.random.uniform(5, 15),
                'age_minutes': np.random.uniform(60, 1440)
            })
        elif anomaly_type == 'oom_kill':
            base_metrics.update({
                'cpu_usage': np.random.uniform(30, 70),
                'memory_usage': np.random.uniform(95, 100),
                'error_rate': np.random.uniform(10, 30),
                'has_oom_kill': 1,
                'ready': 0,
                'phase': 'Failed',
                'container_state': 'Terminated'
            })
        elif anomaly_type == 'crash_loop':
            base_metrics.update({
                'cpu_usage': np.random.uniform(0, 20),
                'memory_usage': np.random.uniform(0, 30),
                'restart_count': np.random.randint(5, 50),
                'has_crash_loop': 1,
                'ready': 0,
                'restart_trend': np.random.uniform(1, 5)
            })
        # Add more anomaly types...
        else:
            # Generic anomaly
            base_metrics.update({
                'cpu_usage': np.random.uniform(50, 90),
                'memory_usage': np.random.uniform(60, 90),
                'error_rate': np.random.uniform(5, 20),
                'latency_ms': np.random.uniform(100, 1000)
            })
        
        return base_metrics
    
    def load_dataset(self, dataset_name: str) -> pd.DataFrame:
        """
        Load a downloaded dataset as pandas DataFrame
        
        Args:
            dataset_name: Name of dataset
            
        Returns:
            DataFrame with loaded data
        """
        dataset_path = self.dataset_dir / dataset_name
        
        if not dataset_path.exists():
            print(f"âš ï¸  Dataset {dataset_name} not found. Downloading...")
            self.download_dataset(dataset_name)
        
        # Try to find CSV file
        csv_files = list(dataset_path.glob("*.csv"))
        if csv_files:
            # Use largest CSV file (usually the main dataset)
            csv_file = max(csv_files, key=lambda x: x.stat().st_size)
            print(f"ðŸ“Š Loading {csv_file}...")
            df = pd.read_csv(csv_file, low_memory=False)
            print(f"âœ… Loaded {len(df)} samples with {len(df.columns)} columns")
            return df
        
        # Try to find other data files
        data_files = list(dataset_path.rglob("*.csv")) + list(dataset_path.rglob("*.json"))
        if data_files:
            # Try loading first file
            data_file = data_files[0]
            print(f"ðŸ“Š Loading {data_file}...")
            if data_file.suffix == '.csv':
                df = pd.read_csv(data_file, low_memory=False)
            elif data_file.suffix == '.json':
                df = pd.read_json(data_file)
            else:
                raise ValueError(f"Unknown file type: {data_file}")
            print(f"âœ… Loaded {len(df)} samples")
            return df
        
        raise FileNotFoundError(f"No data files found in {dataset_path}")
    
    def list_datasets(self) -> Dict:
        """List all available datasets"""
        return DATASET_SOURCES


def main():
    """Download and prepare all datasets"""
    print("="*70)
    print("ðŸš€ BEAST LEVEL Dataset Downloader")
    print("="*70)
    print()
    
    downloader = DatasetDownloader()
    
    print("Available datasets:")
    for name, info in DATASET_SOURCES.items():
        print(f"  â€¢ {name}: {info['name']}")
        print(f"    {info['description']}")
    print()
    
    # Download synthetic K8s dataset (always available, no download needed)
    print("ðŸ“¥ Downloading/Generating datasets...")
    print()
    
    # Generate synthetic K8s dataset (most reliable)
    downloader.download_dataset("synthetic_k8s", force=False)
    
    print()
    print("âœ… Dataset preparation complete!")
    print(f"   Datasets stored in: {DATASET_DIR}")


if __name__ == "__main__":
    main()

